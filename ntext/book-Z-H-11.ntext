we have now considered the elements of programming we have used primitive arithmetic operations we have combined these operations and we have abstracted these composite operations by defining them as compound procedures
but that is not enough to enable us to say that we know how to program
our situation is analogous to that of someone who has learned the rules for how the pieces move in chess but knows nothing of typical openings tactics or strategy
like the novice chess player we do n't yet know the common patterns of usage in the domain
we lack the knowledge of which moves are worth making
we lack the experience to predict the consequences of making a move
the ability to visualize the consequences of the actions under consideration is crucial to becoming an expert programmer just as it is in any synthetic creative activity
in becoming an expert photographer for example one must learn how to look at a scene and know how dark each region will appear on a print for each possible choice of exposure and development conditions
only then can one reason backward planning framing lighting exposure and development to obtain the desired effects
so it is with programming where we are planning the course of action to be taken by a process and where we control the process by means of a program
to become experts we must learn to visualize the processes generated by various types of procedures
only after we have developed such a skill can we learn to reliably construct programs that exhibit the desired behavior
a procedure is a pattern for the local evolution of a computational process
it specifies how each stage of the process is built upon the previous stage
we would like to be able to make statements about the overall or global behavior of a process whose local evolution has been specified by a procedure
this is very difficult to do in general but we can at least try to describe some typical patterns of process evolution
in this section we will examine some common shapes for processes generated by simple procedures
we will also investigate the rates at which these processes consume the important computational resources of time and space
the procedures we will consider are very simple
their role is like that played by test patterns in photography as oversimplified prototypical patterns rather than practical examples in their own right
we begin by considering the factorial function defined by
there are many ways to compute factorials
one way is to make use of the observation that n is equal to n times for any positive integer n
thus we can compute n by computing and multiplying the result by n
if we add the stipulation that one is equal to one this observation translates directly into a procedure
we can use the substitution model of section one point one point five to watch this procedure in action computing six as shown in figure one point three
now let's take a different perspective on computing factorials
we could describe a rule for computing n by specifying that we first multiply one by two then multiply the result by three then by four and so on until we reach n
more formally we maintain a running product together with a counter that counts from one up to n
we can describe the computation by saying that the counter and the product simultaneously change from one step to the next according to the rule
product counter &middot product
counter counter plus one
and stipulating that n is the value of the product when the counter exceeds n
once again we can recast our description as a procedure for computing factorials
as before we can use the substitution model to visualize the process of computing six as shown in figure one point four
compare the two processes
from one point of view they seem hardly different at all
both compute the same mathematical function on the same domain and each requires a number of steps proportional to n to compute n
indeed both processes even carry out the same sequence of multiplications obtaining the same sequence of partial products
on the other hand when we consider the shapes of the two processes we find that they evolve quite differently
consider the first process
the substitution model reveals a shape of expansion followed by contraction indicated by the arrow in figure one point three
the expansion occurs as the process builds up a chain of deferred operations
the contraction occurs as the operations are actually performed
this type of process characterized by a chain of deferred operations is called a recursive process
carrying out this process requires that the interpreter keep track of the operations to be performed later on
in the computation of n the length of the chain of deferred multiplications and hence the amount of information needed to keep track of it grows linearly with n just like the number of steps
such a process is called a linear recursive process
by contrast the second process does not grow and shrink
at each step all we need to keep track of for any n are the current values of the variables product counter and max count
we call this an iterative process
in general an iterative process is one whose state can be summarized by a fixed number of state variables together with a fixed rule that describes how the state variables should be updated as the process moves from state to state and an ( optional ) end test that specifies conditions under which the process should terminate
in computing n the number of steps required grows linearly with n
such a process is called a linear iterative process
the contrast between the two processes can be seen in another way
in the iterative case the program variables provide a complete description of the state of the process at any point
if we stopped the computation between steps all we would need to do to resume the computation is to supply the interpreter with the values of the three program variables
not so with the recursive process
in this case there is some additional hidden information maintained by the interpreter and not contained in the program variables which indicates where the process is in negotiating the chain of deferred operations
the longer the chain the more information must be maintained
in contrasting iteration and recursion we must be careful not to confuse the notion of a recursive process with the notion of a recursive procedure
when we describe a procedure as recursive we are referring to the syntactic fact that the procedure definition refers to the procedure itself
but when we describe a process as following a pattern that is say linearly recursive we are speaking about how the process evolves not about the syntax of how a procedure is written
it may seem disturbing that we refer to a recursive procedure such as fact iter as generating an iterative process
however the process really is iterative its state is captured completely by its three state variables and an interpreter need keep track of only three variables in order to execute the process
one reason that the distinction between process and procedure may be confusing is that most implementations of common languages are designed in such a way that the interpretation of any recursive procedure consumes an amount of memory that grows with the number of procedure calls even when the process described is in principle iterative
as a consequence these languages can describe iterative processes only by resorting to special purpose looping constructs such as do repeat until for and while
the implementation of scheme we shall consider in chapter five does not share this defect
it will execute an iterative process in constant space even if the iterative process is described by a recursive procedure
an implementation with this property is called tail recursive
with a tail recursive implementation iteration can be expressed using the ordinary procedure call mechanism so that special iteration constructs are useful only as syntactic sugar
each of the following two procedures defines a method for adding two positive integers in terms of the procedures inc which increments its argument by one and dec which decrements its argument by one
using the substitution model illustrate the process generated by each procedure in evaluating
are these processes iterative or recursive
the following procedure computes a mathematical function called ackermann's function
what are the values of the following expressions
consider the following procedures where a is the procedure defined above
give concise mathematical definitions for the functions computed by the procedures f g and h for positive integer values of n
for example computes 5n2
another common pattern of computation is called tree recursion
as an example consider computing the sequence of fibonacci numbers in which each number is the sum of the preceding two
in general the fibonacci numbers can be defined by the rule
we can immediately translate this definition into a recursive procedure for computing fibonacci numbers
consider the pattern of this computation
to compute we compute and
to compute we compute and
in general the evolved process looks like a tree as shown in figure one point five
notice that the branches split into two at each level this reflects the fact that the fib procedure calls itself twice each time it is invoked
this procedure is instructive as a prototypical tree recursion but it is a terrible way to compute fibonacci numbers because it does so much redundant computation
notice in figure one point five that the entire computation of almost half the work is duplicated
in fact it is not hard to show that the number of times the procedure will compute or is precisely fib
to get an idea of how bad this is one can show that the value of fib ( n ) grows exponentially with n
more precisely fib ( n ) is the closest integer to n / five where
is the golden ratio which satisfies the equation
thus the process uses a number of steps that grows exponentially with the input
on the other hand the space required grows only linearly with the input because we need keep track only of which nodes are above us in the tree at any point in the computation
in general the number of steps required by a tree recursive process will be proportional to the number of nodes in the tree while the space required will be proportional to the maximum depth of the tree
we can also formulate an iterative process for computing the fibonacci numbers
the idea is to use a pair of integers a and b initialized to fib ( one ) equal one and fib ( 0 ) equal 0 and to repeatedly apply the simultaneous transformations
it is not hard to show that after applying this transformation n times a and b will be equal respectively to fib and fib ( n )
thus we can compute fibonacci numbers iteratively using the procedure
this second method for computing fib ( n ) is a linear iteration
the difference in number of steps required by the two methods one linear in n one growing as fast as fib ( n ) itself is enormous even for small inputs
one should not conclude from this that tree recursive processes are useless
when we consider processes that operate on hierarchically structured data rather than numbers we will find that tree recursion is a natural and powerful tool
but even in numerical operations tree recursive processes can be useful in helping us to understand and design programs
for instance although the first fib procedure is much less efficient than the second one it is more straightforward being little more than a translation into lisp of the definition of the fibonacci sequence
to formulate the iterative algorithm required noticing that the computation could be recast as an iteration with three state variables
it takes only a bit of cleverness to come up with the iterative fibonacci algorithm
in contrast consider the following problem how many different ways can we make change of $ 1.00 given half dollars quarters dimes nickels and pennies
more generally can we write a procedure to compute the number of ways to change any given amount of money
this problem has a simple solution as a recursive procedure
suppose we think of the types of coins available as arranged in some order
then the following relation holds
the number of ways to change amount a using n kinds of coins equals
the number of ways to change amount a using all but the first kind of coin plus
the number of ways to change amount a minus d using all n kinds of coins where d is the denomination of the first kind of coin
to see why this is true observe that the ways to make change can be divided into two groups those that do not use any of the first kind of coin and those that do
therefore the total number of ways to make change for some amount is equal to the number of ways to make change for the amount without using any of the first kind of coin plus the number of ways to make change assuming that we do use the first kind of coin
but the latter number is equal to the number of ways to make change for the amount that remains after using a coin of the first kind
thus we can recursively reduce the problem of changing a given amount to the problem of changing smaller amounts using fewer kinds of coins
consider this reduction rule carefully and convince yourself that we can use it to describe an algorithm if we specify the following degenerate cases
if a is exactly 0 we should count that as one way to make change
if a is less than 0 we should count that as 0 ways to make change
if n is 0 we should count that as 0 ways to make change
we can easily translate this description into a recursive procedure
we can now answer our original question about changing a dollar
count change generates a tree recursive process with redundancies similar to those in our first implementation of fib
on the other hand it is not obvious how to design a better algorithm for computing the result and we leave this problem as a challenge
the observation that a tree recursive process may be highly inefficient but often easy to specify and understand has led people to propose that one could get the best of both worlds by designing a smart compiler that could transform tree recursive procedures into more efficient procedures that compute the same result
a function f is defined by the rule that f ( n ) equal n if n three and f ( n ) equal f plus 2f plus 3f if n three
write a procedure that computes f by means of a recursive process
write a procedure that computes f by means of an iterative process
the following pattern of numbers is called pascal's triangle
the numbers at the edge of the triangle are all one and each number inside the triangle is the sum of the two numbers above it
write a procedure that computes elements of pascal's triangle by means of a recursive process
prove that fib ( n ) is the closest integer to n / five where equal / two
hint let equal / two
use induction and the definition of the fibonacci numbers to prove that fib ( n ) equal / five
the previous examples illustrate that processes can differ considerably in the rates at which they consume computational resources
one convenient way to describe this difference is to use the notion of order of growth to obtain a gross measure of the resources required by a process as the inputs become larger
let n be a parameter that measures the size of the problem and let r ( n ) be the amount of resources the process requires for a problem of size n
in our previous examples we took n to be the number for which a given function is to be computed but there are other possibilities
for instance if our goal is to compute an approximation to the square root of a number we might take n to be the number of digits accuracy required
for matrix multiplication we might take n to be the number of rows in the matrices
in general there are a number of properties of the problem with respect to which it will be desirable to analyze a given process
similarly r ( n ) might measure the number of internal storage registers used the number of elementary machine operations performed and so on
in computers that do only a fixed number of operations at a time the time required will be proportional to the number of elementary machine operations performed
we say that r ( n ) has order of growth written r ( n ) equal if there are positive constants k one and k two independent of n such that
for any sufficiently large value of n
for instance with the linear recursive process for computing factorial described in section one point two point one the number of steps grows proportionally to the input n
thus the steps required for this process grows as ( n )
we also saw that the space required grows as ( n )
for the iterative factorial the number of steps is still ( n ) but the space is ( one ) that is constant
the tree recursive fibonacci computation requires ( n ) steps and space ( n ) where is the golden ratio described in section one point two point two
orders of growth provide only a crude description of the behavior of a process
for example a process requiring n two steps and a process requiring 1000n2 steps and a process requiring 3n2 plus 10n plus seventeen steps all have ( n two ) order of growth
on the other hand order of growth provides a useful indication of how we may expect the behavior of the process to change as we change the size of the problem
for a ( n ) ( linear ) process doubling the size will roughly double the amount of resources used
for an exponential process each increment in problem size will multiply the resource utilization by a constant factor
in the remainder of section one point two we will examine two algorithms whose order of growth is logarithmic so that doubling the problem size increases the resource requirement by a constant amount
draw the tree illustrating the process generated by the count change procedure of section one point two point two in making change for eleven cents
what are the orders of growth of the space and number of steps used by this process as the amount to be changed increases
the sine of an angle can be computed by making use of the approximation sin x x if x is sufficiently small and the trigonometric identity
to reduce the size of the argument of sin
these ideas are incorporated in the following procedures
a
how many times is the procedure p applied when is evaluated
b
what is the order of growth in space and number of steps used by the process generated by the sine procedure when is evaluated
consider the problem of computing the exponential of a given number
we would like a procedure that takes as arguments a base b and a positive integer exponent n and computes bn
one way to do this is via the recursive definition
which translates readily into the procedure
this is a linear recursive process which requires ( n ) steps and ( n ) space
just as with factorial we can readily formulate an equivalent linear iteration
this version requires ( n ) steps and ( one ) space
we can compute exponentials in fewer steps by using successive squaring
for instance rather than computing b eight as
we can compute it using three multiplications
this method works fine for exponents that are powers of two
we can also take advantage of successive squaring in computing exponentials in general if we use the rule
we can express this method as a procedure
where the predicate to test whether an integer is even is defined in terms of the primitive procedure remainder by
the process evolved by fast expt grows logarithmically with n in both space and number of steps
to see this observe that computing b2n using fast expt requires only one more multiplication than computing bn
the size of the exponent we can compute therefore doubles ( approximately ) with every new multiplication we are allowed
thus the number of multiplications required for an exponent of n grows about as fast as the logarithm of n to the base two
the process has growth
the difference between growth and ( n ) growth becomes striking as n becomes large
for example fast expt for n equal 1000 requires only fourteen multiplications
it is also possible to use the idea of successive squaring to devise an iterative algorithm that computes exponentials with a logarithmic number of steps although as is often the case with iterative algorithms this is not written down so straightforwardly as the recursive algorithm
design a procedure that evolves an iterative exponentiation process that uses successive squaring and uses a logarithmic number of steps as does fast expt
the exponentiation algorithms in this section are based on performing exponentiation by means of repeated multiplication
in a similar way one can perform integer multiplication by means of repeated addition
the following multiplication procedure is analogous to the expt procedure
this algorithm takes a number of steps that is linear in b
now suppose we include together with addition operations double which doubles an integer and halve which divides an ( even ) integer by two
using these design a multiplication procedure analogous to fast expt that uses a logarithmic number of steps
using the results of exercises one point sixteen and one point seventeen devise a procedure that generates an iterative process for multiplying two integers in terms of adding doubling and halving and uses a logarithmic number of steps
there is a clever algorithm for computing the fibonacci numbers in a logarithmic number of steps
recall the transformation of the state variables a and b in the fib iter process of section one point two point two a a plus b and b a
call this transformation t and observe that applying t over and over again n times starting with one and 0 produces the pair fib and fib ( n )
in other words the fibonacci numbers are produced by applying tn the nth power of the transformation t starting with the pair ( one 0 )
now consider t to be the special case of p equal 0 and q equal one in a family of transformations tpq where tpq transforms the pair ( a b ) according to a bq plus aq plus ap and b bp plus aq
show that if we apply such a transformation tpq twice the effect is the same as using a single transformation tp'q' of the same form and compute p' and q' in terms of p and q
this gives us an explicit way to square these transformations and thus we can compute tn using successive squaring as in the fast expt procedure
put this all together to complete the following procedure which runs in a logarithmic number of steps
the greatest common divisor ( gcd ) of two integers a and b is defined to be the largest integer that divides both a and b with no remainder
for example the gcd of sixteen and twenty eight is four
in chapter two when we investigate how to implement rational number arithmetic we will need to be able to compute gcds in order to reduce rational numbers to lowest terms
one way to find the gcd of two integers is to factor them and search for common factors but there is a famous algorithm that is much more efficient
the idea of the algorithm is based on the observation that if r is the remainder when a is divided by b then the common divisors of a and b are precisely the same as the common divisors of b and r
thus we can use the equation
to successively reduce the problem of computing a gcd to the problem of computing the gcd of smaller and smaller pairs of integers
for example
reduces gcd ( two hundred and six forty ) to gcd ( two 0 ) which is two
it is possible to show that starting with any two positive integers and performing repeated reductions will always eventually produce a pair where the second number is 0
then the gcd is the other number in the pair
this method for computing the gcd is known as euclid's algorithm
it is easy to express euclid's algorithm as a procedure
this generates an iterative process whose number of steps grows as the logarithm of the numbers involved
the fact that the number of steps required by euclid's algorithm has logarithmic growth bears an interesting relation to the fibonacci numbers
lam&eacute 's theorem if euclid's algorithm requires k steps to compute the gcd of some pair then the smaller number in the pair must be greater than or equal to the kth fibonacci number
we can use this theorem to get an order of growth estimate for euclid's algorithm
let n be the smaller of the two inputs to the procedure
if the process takes k steps then we must have n> fib ( k ) k / five
therefore the number of steps k grows as the logarithm of n
hence the order of growth is
the process that a procedure generates is of course dependent on the rules used by the interpreter
as an example consider the iterative gcd procedure given above
suppose we were to interpret this procedure using normal order evaluation as discussed in section one point one point five
using the substitution method illustrate the process generated in evaluating and indicate the remainder operations that are actually performed
how many remainder operations are actually performed in the normal order evaluation of
in the applicative order evaluation
this section describes two methods for checking the primality of an integer n one with order of growth ( n ) and a probabilistic algorithm with order of growth
the exercises at the end of this section suggest programming projects based on these algorithms
since ancient times mathematicians have been fascinated by problems concerning prime numbers and many people have worked on the problem of determining ways to test if numbers are prime
one way to test if a number is prime is to find the number's divisors
the following program finds the smallest integral divisor of a given number n
it does this in a straightforward way by testing n for divisibility by successive integers starting with two
we can test whether a number is prime as follows n is prime if and only if n is its own smallest divisor
the end test for find divisor is based on the fact that if n is not prime it must have a divisor less than or equal to n
this means that the algorithm need only test divisors between one and n
consequently the number of steps required to identify n as prime will have order of growth ( n )
the primality test is based on a result from number theory known as fermat's little theorem
fermat's little theorem if n is a prime number and a is any positive integer less than n then a raised to the nth power is congruent to a modulo n
if n is not prime then in general most of the numbers a < n will not satisfy the above relation
this leads to the following algorithm for testing primality given a number n pick a random number a < n and compute the remainder of an modulo n
if the result is not equal to a then n is certainly not prime
if it is a then chances are good that n is prime
now pick another random number a and test it with the same method
if it also satisfies the equation then we can be even more confident that n is prime
by trying more and more values of a we can increase our confidence in the result
this algorithm is known as the fermat test
to implement the fermat test we need a procedure that computes the exponential of a number modulo another number
this is very similar to the fast expt procedure of section one point two point four
it uses successive squaring so that the number of steps grows logarithmically with the exponent
the fermat test is performed by choosing at random a number a between one and n minus one inclusive and checking whether the remainder modulo n of the nth power of a is equal to a
the random number a is chosen using the procedure random which we assume is included as a primitive in scheme
random returns a nonnegative integer less than its integer input
hence to obtain a random number between one and n minus one we call random with an input of n minus one and add one to the result
the following procedure runs the test a given number of times as specified by a parameter
its value is true if the test succeeds every time and false otherwise
the fermat test differs in character from most familiar algorithms in which one computes an answer that is guaranteed to be correct
here the answer obtained is only probably correct
more precisely if n ever fails the fermat test we can be certain that n is not prime
but the fact that n passes the test while an extremely strong indication is still not a guarantee that n is prime
what we would like to say is that for any number n if we perform the test enough times and find that n always passes the test then the probability of error in our primality test can be made as small as we like
unfortunately this assertion is not quite correct
there do exist numbers that fool the fermat test numbers n that are not prime and yet have the property that an is congruent to a modulo n for all integers a < n
such numbers are extremely rare so the fermat test is quite reliable in practice
there are variations of the fermat test that cannot be fooled
in these tests as with the fermat method one tests the primality of an integer n by choosing a random integer a <n and checking some condition that depends upon n and a
on the other hand in contrast to the fermat test one can prove that for any n the condition does not hold for most of the integers a <n unless n is prime
thus if n passes the test for some random choice of a the chances are better than even that n is prime
if n passes the test for two random choices of a the chances are better than three out of four that n is prime
by running the test with more and more randomly chosen values of a we can make the probability of error as small as we like
the existence of tests for which one can prove that the chance of error becomes arbitrarily small has sparked interest in algorithms of this type which have come to be known as probabilistic algorithms
there is a great deal of research activity in this area and probabilistic algorithms have been fruitfully applied to many fields
use the smallest divisor procedure to find the smallest divisor of each of the following numbers one hundred ninety nine 1999 19999
most lisp implementations include a primitive called runtime that returns an integer that specifies the amount of time the system has been running
the following timed prime test procedure when called with an integer n prints n and checks to see if n is prime
if n is prime the procedure prints three asterisks followed by the amount of time used in performing the test
using this procedure write a procedure search for primes that checks the primality of consecutive odd integers in a specified range
use your procedure to find the three smallest primes larger than 1000 larger than ten 0 hundred larger than one hundred 000 larger than one 0 hundred 000
note the time needed to test each prime
since the testing algorithm has order of growth of ( n ) you should expect that testing for primes around ten 0 hundred should take about ten times as long as testing for primes around 1000
do your timing data bear this out
how well do the data for one hundred 000 and one 0 hundred 000 support the n prediction
is your result compatible with the notion that programs on your machine run in time proportional to the number of steps required for the computation
the smallest divisor procedure shown at the start of this section does lots of needless testing after it checks to see if the number is divisible by two there is no point in checking to see if it is divisible by any larger even numbers
this suggests that the values used for test divisor should not be two three four five six ... but rather two three five seven nine ...
to implement this change define a procedure next that returns three if its input is equal to two and otherwise returns its input plus two
modify the smallest divisor procedure to use instead of
with timed prime test incorporating this modified version of smallest divisor run the test for each of the twelve primes found in exercise one point twenty two
since this modification halves the number of test steps you should expect it to run about twice as fast
is this expectation confirmed
if not what is the observed ratio of the speeds of the two algorithms and how do you explain the fact that it is different from two
modify the timed prime test procedure of exercise one point twenty two to use fast prime and test each of the twelve primes you found in that exercise
since the fermat test has growth how would you expect the time to test primes near one 0 hundred 000 to compare with the time needed to test primes near 1000
do your data bear this out
can you explain any discrepancy you find
alyssa p
hacker complains that we went to a lot of extra work in writing expmod
after all she says since we already know how to compute exponentials we could have simply written
is she correct
would this procedure serve as well for our fast prime tester
explain
louis reasoner is having great difficulty doing exercise one point twenty four
his fast prime test seems to run more slowly than his prime test
louis calls his friend eva lu ator over to help
when they examine louis's code they find that he has rewritten the expmod procedure to use an explicit multiplication rather than calling square
i do n't see what difference that could make says louis
i do . says eva
by writing the procedure like that you have transformed the process into a ( n ) process . explain
demonstrate that the carmichael numbers listed in footnote forty seven really do fool the fermat test
that is write a procedure that takes an integer n and tests whether an is congruent to a modulo n for every a <n and try your procedure on the given carmichael numbers
one variant of the fermat test that cannot be fooled is called the miller rabin test
this starts from an alternate form of fermat's little theorem which states that if n is a prime number and a is any positive integer less than n then a raised to the st power is congruent to one modulo n
to test the primality of a number n by the miller rabin test we pick a random number a <n and raise a to the st power modulo n using the expmod procedure
however whenever we perform the squaring step in expmod we check to see if we have discovered a nontrivial square root of one modulo n that is a number not equal to one or n minus one whose square is equal to one modulo n
it is possible to prove that if such a nontrivial square root of one exists then n is not prime
it is also possible to prove that if n is an odd number that is not prime then for at least half the numbers a <n computing an one in this way will reveal a nontrivial square root of one modulo n
modify the expmod procedure to signal if it discovers a nontrivial square root of one and use this to implement the miller rabin test with a procedure analogous to fermat test
check your procedure by testing various known primes and non primes
hint one convenient way to make expmod signal is to have it return 0
